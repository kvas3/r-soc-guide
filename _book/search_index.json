[["index.html", "R Guide for Sociological Analyses 1 About 1.1 Why you should learn R 1.2 Usage", " R Guide for Sociological Analyses Vasudha Kumar, Daniel Lavarte 2023-08-22 1 About Welcome to this guide! We, Vas (c/o 2021) and Daniel (c/o 2022), are two sociology alumni from Santa Clara. One of the highlights of our time in the department was our RAship with Dr. Molly King. We loved our time learning R under Prof. King’s guidance and are excited to pass on our learnings to support future RAs with their research. We hope this guide will help you get excited and set-up to conduct sociological analyses in R! 1.1 Why you should learn R R is a free, open-source statistical software. It’s widely used in academia &amp; research and is also used in many industries for data analysis, visualization, and GIS. If you don’t know other programming/coding languages yet, R is a great primer as it translates well to Python and will help you develop data science and computational social science skills. Because R is open-source, it makes our scientific pursuits of reproducibility, transparency, and open science much easier. Last but not least, there is a large community of users and developers who contribute to improving R, so it’s very likely that other users have already encountered the errors and roadblocks that might line your path as you learn this language. 1.2 Usage This guide will set you up to conduct research with Dr. King in R, but it assumes some basic knowledge of R. Here are some resources to familiarize yourself with R syntax before you use our guide. You don’t have to go through all of them, but pick 1-2 to get started. Hands-on Programming with R (book; see appendix for R installation instructions) Impatient R (quick written tutorial) Self-paced workshops by UC Berkeley’s D-Lab R Fundamentals Data Wrangling in R Data Visualization Data Manipulation in R by Steph Locke (book) A Gentle Introduction to Tidy Statistics in R (webinar) R Cookbook by James Long (book) Survey Data Analysis with R (webpage) 1.2.1 Notes Vas and Daniel started learning R in 2020, and this guide was last updated in 2023. R is a powerful language, so there is all sorts of analysis you can leverage it for. To name a few, spatial research (e.g., mapping and GIS), textual analysis, and network analysis can all be performed using R. This guide covers one small part: survey analysis, specifically, analysis of large-scale survey data that is weighted. Fortunately, the datasets we worked with during our RAships with Dr. King had already been meticulously cleaned, but it’s possible you’ll have to clean and tidy data on your own. We hope the resources above can help you get started here. R is open-source, so this means there are always new updates and features added to the software to ensure that it keeps up with the times. It’s possible that some of the code in this guide (or content in the resources listed above) is outdated. With a little googling, you should be able find tons of other learning materials online. You’re always welcome to email vasudha.037@gmail.com with questions, corrections, or other musings. "],["data-set-up.html", "2 Data Set-Up 2.1 Package Installs 2.2 Working Directory 2.3 Backups 2.4 Data Import", " 2 Data Set-Up 2.1 Package Installs If you find yourself on this page but have have yet to work through any of the resources on the previous page (good sign! you’re excited to learn!), try to at least work through this webpage before following this guide. Our guide won’t be able to teach you the fundamentals of R, and skipping the basics now may haunt you later! Now that you have hopefully gone through at least one introductory resource, you are better prepared to use this guide! To get started, open RStudio and open a new R script. As a reminder, you can open a script by clicking File -&gt; New File -&gt; R Script once RStudio is open. The first step for your research project will be to install and load the necessary packages. As you may have read here, a package is just a set of functions (or tools) to help you analyze data in R. You need to download each package just once, and you can do so from R’s command line (remember: it’s that bottom line in the Console panel where you can type code) directly. Once you download a package, you have to load it into your R session. This makes sure that the package is ready for your use, and you have to do this every time you start a new R session. Here are some packages Vas and Daniel used in their research: tidyverse: This is probably one of the most used packages in the R-verse. It’s actually a package of packages and will be necessary for data tidying, manipulation, and visualization. survey: This package is useful for analyzing large-scale survey data. We’ll use to add weights to our data and develop descriptive statistics including means, ratios, etc. haven: If the data set you are using is a .dta file (a commonly used format for Stata users), you’ll need this package to allow R to read this type of file format. To install these packages, simply run: install.packages(&quot;tidyverse&quot;) install.packages(&quot;survey&quot;) install.packages(&quot;haven&quot;) # Remember, you need to do this just once on your computer. To load these packages into your R session, run: library(&quot;haven&quot;) library(&quot;survey&quot;) library(&quot;tidyverse&quot;) # You&#39;ll need to run these lines of code every time you start a new R session. # It&#39;s a good idea to paste them into your R script with all the other code # you save for your use later. 2.2 Working Directory Refresher: a working directory is “the default location where R will look for files you want to load and where it will put any files you save” (Douglas et al 2023). You need to set your working directory correctly in order to tell R the location of your survey data on your computer Refresher on how to set your working directory. Before setting your directory, we recommend that your datasets and all other files related to your research project are available in one dedicated folder on your computer. You can set your working directory by following the code template below: setwd(&quot;/Users/vkumar/Desktop/finlit_project&quot;) setwd is the function used to set our directory. \"/Users/vkumar/Desktop/finlit_project\" is the path to the folder where Vas stored all her project data. If using MacOS, you can find this filepath by right-clicking on your project folder, then holding down the option key on your keyboard, then clicking on “Copy [FOLDER] as Pathname. 2.3 Backups Research is hard work, and we know all too well the pain of losing hours of work to computer crashes and other nightmares. Before you make any more progress with your research project, make sure you have dedicated folder to store all your code and data. Then, set up an automatic backup system (since the code is the only part of the project you need to reproduce the analysis, the code is a very efficient way to back up your project!). Google Backup and Sync is a good way to backup automatically to Google Drive. Set this up to automatically backup to a folder in your Google Drive RA folder shared with Dr. King. GitHub is another (probably better) option, but since it is has somewhat of a learning curve, we won’t cover it here. Here’s one resource to help you get started. 2.4 Data Import You’re so close! All that’s left for you to start conducting your amazing research is importing your data into R. If your data is in csv (spreadsheet) format, you can run something like this to import it into R: mydata &lt;- read_csv(&quot;myfile.csv&quot;) # Here, &quot;mydata&quot; is what we named our dataset in the R workspace. # Now, whenever we want to work with our data, we just need to call it # by its name, &quot;mydata&quot;. You can name your data whatever you&#39;d like. # &quot;myfile.csv&quot; is the name of our dataset in our working directory Prof. King uses the Stata software to conduct her research, so it’s likely that you’ll be working with files in the dta format. Luckily, there’s a package that lets us import Stata files into R. The code template is very similar to the one above, except the function below relies on the haven package that we installed earlier. mydata &lt;- read_dta(&quot;myfile.dta&quot;) Excellent! We’re ready for some analysis! "],["basic-descriptive-statistics.html", "3 Basic Descriptive Statistics 3.1 Calculating Means 3.2 Adding Survey Weights 3.3 Using the Survey Package", " 3 Basic Descriptive Statistics When you start performing your own sociological analysis, you’ll use own dataset that you imported in the last chapter. For demonstration purposes, we’ll use an example dataset that comes along with the survey package. Feel free to follow along - all the code below will also work on your machine as long the survey package is loaded in your R session. To load the package and the example datasets into your R workspace, run: library(survey) data(api) You should now have several datasets loaded into you RStudio workspace. These contain data on Academic Performance Index (API), a measure of student performance for California schools for 1999 and 2000. Specifically, we’ll be working with the apistrat dataset in RStudio. Learn more about this example dataset and its variables in the survey package documentation (p 5-7). 3.1 Calculating Means If you skimmed through the resource materials we provided at the start of this guide, you probably already know that R makes it extremely easy for you to calculate the mean - or average - of a variable. Let’s say we want the average of the api00, the API (Academic Performance Index) in 2000. We could run the code below which tells R to compute the mean of the api00 variable in the apipop dataset. mean(apipop$api00) Sidenote: Another way to calculate the mean is to run the code below: apipop %&gt;% summarize(mean(api00)) Sidenote: both of these chunks of code give you the same output. R is very flexible in its use, so there will often be several ways to achieve the same output. This might feel overwhelming in the beginning. For now, you don’t have to keep track of everything or learn every single method. Stick to learning just one method for the tasks you want to accomplish. As you become more comfortable with R syntax, you’ll be able to understand other methods and adapt to different syntax more easily. Great! We now know that the average API in 2000 was 664.71! When it comes to survey data, however, it is rarely accurate to just calculate the simple mean of your data. Survey data is often based on a sample of the population, which means that there are usually weights to each response (or row) in the dataset. In our case, the apistrat dataset provides probability weights in the pw column. 3.2 Adding Survey Weights The first step in our survey analysis process is to add weights to the data frame. For this, we’re relying on the survey package described in the previous chapter. The svydesign function from this package allows us to create a new dataset with the added weights. data.w &lt;- svydesign(ids = ~0, data = apistrat, weights = ~pw) Hopefully, this code is straightforward enough for you to decipher. If you’re unsure about any arguments (especially ids), take a look at the documentation for this package to help yourself figure them out. You can also run ?svydesign in RStudio to view the help page for the function. Reading function documentation to pull out the information you need - no matter how complex it may all seem - is an important coding skill! Honing it early will be helpful, so make it a habit to skim through help pages for commonly-used functions. Note that we add the ~ symbol (called a tilde) when specifying weights because that’s the syntax followed by the survey package. This means that this package generally requires the tilde when specifying variable names or R will return an error. 3.3 Using the Survey Package Great, now we’re ready to do some real (weighted) analysis! 3.3.1 Weighted Means The survey package provides specific functions to compute descriptive statistics for weighted, survey data. We’ll use the svymean function to compute some means. The svymean function takes two required arguments: (1) the variable for which to compute the mean, and (2) the weighted dataset. Let’s try. svymean(~api00, design = data.w) This returns the mean of the API in 2000 for California schools. If the dataset you’re working with has missing values, R may return an error or NAs. You can fix this using the additional argument na.rm for all functions in this package. svymean(~api00, design = data.w, na.rm = TRUE) This ensures that missing values are omitted while computing the means. If your data has missing values, discuss with Dr. King to decide the best way to deal with them. Note that the dataset data.w we provide in this command is the modified, weighted data, not the original unweighted data. You can also leave out the argument names, a.k.a design =, and simply type data.w after the comma, but it is a good idea to practice including argument names in the beginning. 3.3.2 Working with several variables When conducting your research, it’s likely that your research question will concern several variables. Taking the example of the API dataset, we might be interested in changes in the Academic Performance Index from 1999 to 2000. To calculate the mean API for both years, we could run: svymean(~api99, design = data.w) svymean(~api00, design = data.w) While this gives us the information we wanted, it can sometimes be annoying to type out similar commands repeatedly. Fortunately, the survey packages allows us to calculate the means of several variable simultaneously. Simply separate your variables of interest using a plus (+) sign. svymean(~api99+api00, design = data.w) 3.3.3 Quantiles Next, let’s try to compute quantiles and medians for this data using the svyquantile function. This function takes an extra required argument called “quantiles” to tell R the percentiles you want to compute. In this example, let’s first try computing the median (or 50th percentile) of the API in 2000. svyquantile(~api00, design = data.w, quantiles = 0.5) Read the output carefully. It provides us several pieces of key information: quantile, ci.2.5, ci.97.5, and se. Try to read the svyquantile function’s documentation by running ?svyquantile to understand what these data mean. You can also compute several quantiles simultaneously. The following line of code computes the 25, 50, and 75th percentiles. svyquantile(~api00, design = data.w, quantiles = c(0.25, 0.5, 0.75)) Like in svymean, you can compute the quantiles (and other survey statistics) for more than one variable using the plus sign. svyquantile(~api99+api00, design = data.w, quantiles = c(0.25, 0.5, 0.75)) Great work! On your own, try calculating every 10th percentile of the API in the years 1999 and 2000. 3.3.4 Variance Next, let’s try computing variance. We’ll use the svyvar function that relies on the same structure as the functions above. svyvar(~api99+api00, design = data.w) 3.3.5 Totals This next function, svytotal also uses a similar format to estimate population totals for a variable. Let’s try to figure out the number of students enrolled in each level (E, M, H) of California schools. svytotal(~stype, design = data.w) ## total SE ## stypeE 4421 313.40 ## stypeH 755 92.70 ## stypeM 1018 124.99 Notice how the unweighted data only has a total of 200 schools, yet our weighted data returns thousands of schools. This is because of our survey weights. 3.3.6 Statistics for subsets of data For your research question, you are most likely most interested in the relationship between two or more variables. In this data, we might be interested in exploring the API across various school types, i.e., elementary, middle, and high schools. We can do such computations using the `svyby` function. Let’s estimate the mean API in 2000 across the different levels/types of schools (E. M, H). svyby(~api00, by = ~stype, design = data.w, FUN = svymean) ## stype api00 se ## E E 674.43 12.49343 ## H H 625.82 15.34078 ## M M 636.60 16.50239 Here, we are asking R to compute the mean API in 2000 by the type of school. You can calculate different statistics by providing different functions to the FUN argument. Try calculating the variance of API in 2000 by school type. (Hint: you would replace something in the code above with svyvar.) Recall that you can calculate the mean of several variables across several variables using the + sign. The code below returns the API means for 1999 and 2000 categorized by the school level and year-round status. svyby(~api99+api00, by = ~stype+yr.rnd, design = data.w, FUN = svymean) ## stype yr.rnd api99 api00 se.api99 se.api00 ## E.No E No 659.5976 695.4024 14.15177 13.23504 ## H.No H No 620.0816 628.8571 15.76721 15.34873 ## M.No M No 614.5625 641.2708 16.91108 16.68175 ## E.Yes E Yes 527.7778 578.8889 22.63897 23.67867 ## H.Yes H Yes 484.0000 477.0000 0.00000 0.00000 ## M.Yes M Yes 505.5000 524.5000 49.26724 57.77382 The output also reports the standard errors. You can ask to report confidence intervals or other measures of variability by adding the argument vartype as below: svyby(~api99+api00, by = ~stype+yr.rnd, design = data.w, FUN = svymean, vartype = &quot;ci&quot;) ## stype yr.rnd api99 api00 ci_l.api99 ci_l.api00 ci_u.api99 ## E.No E No 659.5976 695.4024 631.8606 669.4622 687.3345 ## H.No H No 620.0816 628.8571 589.1785 598.7742 650.9848 ## M.No M No 614.5625 641.2708 581.4174 608.5752 647.7076 ## E.Yes E Yes 527.7778 578.8889 483.4062 532.4796 572.1493 ## H.Yes H Yes 484.0000 477.0000 484.0000 477.0000 484.0000 ## M.Yes M Yes 505.5000 524.5000 408.9380 411.2654 602.0620 ## ci_u.api00 ## E.No 721.3426 ## H.No 658.9401 ## M.No 673.9665 ## E.Yes 625.2982 ## H.Yes 477.0000 ## M.Yes 637.7346 3.3.7 Saving outputs While conducting your research and analysis, it’s likely that you’ll use the values you estimated above several times (e.g., to compare with other means, to create figures and charts, to export in a spreadsheet, etc.) Instead of computing these values every time you want to conduct analyses, you can save them as an ‘object’ in R. Do this using the &lt;- symbol. Use an object name/abbreviation that you can be consistent with and recall easily. means &lt;- svyby(~api99+api00, by = ~stype+yr.rnd, design = data.w, FUN = svymean, vartype = &quot;ci&quot;) Now, whenever you want to take a look at this table, you can view the means table and use it to run other analyses. If other functions ever give you trouble when running analyses on the means object due to its “object type” being “svyby” (you can see this in the right-hand Environment pane in RStudio), you can also convert it to the data frame format by running means &lt;- as.data.frame(means). "],["plotting.html", "4 Plotting 4.1 Saving figures", " 4 Plotting The ggplot2 package is standard in the R universe to create high quality plots and graphs. Below is a brief overview, but you should also take a look at the ggplot2 cheatsheet. The cheatsheet will be useful when you want greater control over plot formatting, scales, color, error bars, and other extras later. The ggplot2 package comes as part of the tidyverse. If you didn’t install the tidyverse package earlier, do it now using install.packages(\"tidyverse\") and then load it into your R workspace using library. 4.0.1 Using ggplot2 When creating graphs using ggplot, the first line of code always uses the function ggplot() This is the foundation of your plot where you will usually provide the data for the graph, the variables you want to plot, and other aesthetic factors like colors. ggplot(data = means, mapping = aes(x = stype, y = api00)) Here, we’re plotting the type of school (E, M, H) on the x-axis and the mean API score in 2000 on the y-axis. Running this code alone will give you an empty plot. Every time you want to add a new layer to graph (e.g. line plots, bar graphs) you’ll use a plus sign at the end of each line. The extra layers to plot your data are called geoms. Below we use the point geom to display our data as points on the graph. ggplot(data = means, mapping = aes(x = stype, y = api00)) + geom_point() Recall that our table features each category of school twice: once for schools that are year round and another time for schools that are not, which is why this plot gives you two points/values for each category. We can differentiate between the two by adding a color argument in the plot aesthetics. ggplot(data = means, mapping = aes(x = stype, y = api00, color = yr.rnd)) + geom_point() This code groups the points using the yr.rnd variable using colors and adds a legend. You can achieve something similar by using shape instead of color as well. Something that doesn’t quite work in our plot is the order of the school type variable. R orders the schools alphabetically: Elementary, High, then Middle. But we know that there’s an order/heirarchy to how this should be displayed. You can add an order to any variable using the ordered function. means$stype &lt;- ordered(means$stype, levels = c(&quot;E&quot;, &quot;M&quot;, &quot;H&quot;)) Note that the $ sign after a data table name is used to specify a column in a dataframe. Now try plotting your graph again to check if it changes. While this chart looks nice, our data is probably better suited to a bar chart. Try making one using geom_bar instead of geom_point. Does your code return an error? This is because geom_bar() on its own is used to display histograms (which require only one variable). To ensure that R knows we want a bar graph as the output, add the stat argument to the bar geom layer as below: ggplot(data = means, mapping = aes(x = stype, y = api00, fill = yr.rnd)) + geom_bar(stat = &quot;identity&quot;) (We changed our color argument in the plot aesthetics to fill because color will only outline the shapes instead of filling them with color.) This plot is still not quite what we want because our bars for each school category are stacked on top of each other. To modify with the positioning of these bars, we use the position argument in the geom layer. ggplot(data = means, mapping = aes(x = stype, y = api00, fill = yr.rnd)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) This is a better graph to interpret and draw inferences from. Other formatting options would be to change the labels of the axes, the legend titles, etc. You can layer multiple geom layers on the same plot. For example, we can add error bars to this graph using the confidence intervals we computed earlier. To add error bars: ggplot(data = means, mapping = aes(x = stype, y = api00, fill = yr.rnd)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(mapping = aes(ymin = ci_l.api00, ymax = ci_u.api00, width = 0.2), position = position_dodge(0.9)) Here, we use ymin and ymax to specify where the lower and upper points of the confidence interval lines should appear. These variables already exist in our data. Great job! This is a lot of code to understand in the beginning, but you can find all these options along with additional geom layers, scale options, coordinate systems, etc in the ggplot cheatsheet. 4.1 Saving figures Now, let’s save this chart to our computer, so you can reference it easily later. ggsave(filename = &quot;api00_stype.png&quot;) Where on your computer do you think you just saved this plot? Hint: think about your working directory! "],["conclusion.html", "5 Conclusion", " 5 Conclusion If you worked your work through this guide, congratulations! R can have a steep learning curve, so remember to be patient with yourself. This guide provides only a glimpse into the analyses you can run in R, but we hope it helps you get started with survey data! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
